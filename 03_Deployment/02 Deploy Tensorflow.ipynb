{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Introduction\n",
    "In this workbook, we will train a simple Tensorflow model and deploy that for inference. \n",
    "In this example, we use TensorFlow's [premade estimator iris data example](https://www.tensorflow.org/tutorials/estimator/premade) and add MLflow tracking.\n",
    "This example trains a `tf.estimator.DNNClassifier` on the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) and predicts on a validation set.\n",
    "We then demonstrate how to load the saved model back as a generic `mlflow.pyfunc`, allowing us to make predictions.\n",
    "\n",
    "\n",
    "## 2. Imports and Dependencies.\n",
    "The few packages needed are loaded next. Particularly, `tensorflow`, `mlflow` will be majorly used in this tutorial. `requests` package will be used for performing query. `json` is used to post and get response from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 23:40:49.979783: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-13 23:40:49.979819: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import mlflow.tensorflow\n",
    "from mlflow import pyfunc\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Setting a tracking uri to log the mlflow logs in a particular location tracked by \n",
    "from mlflow.tracking import MlflowClient\n",
    "tracking_uri = os.environ.get(\"TRACKING_URL\")\n",
    "client = MlflowClient(tracking_uri=tracking_uri)\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "# Setting a tracking project experiment name to keep the experiments organized\n",
    "experiments = client.list_experiments()\n",
    "experiment_names = []\n",
    "for exp in experiments:\n",
    "    experiment_names.append(exp.name)\n",
    "experiment_name = \"tf_development\"\n",
    "if experiment_name not in experiment_names:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility function to add libraries to conda environment\n",
    "def add_libraries_to_conda_env(_conda_env,libraries=[],conda_dependencies=[]):\n",
    "    dependencies = _conda_env[\"dependencies\"]\n",
    "    dependencies = dependencies + conda_dependencies\n",
    "    pip_index = None\n",
    "    for _index,_element in enumerate(dependencies):\n",
    "        if type(_element) == dict:\n",
    "            if \"pip\" in _element.keys():\n",
    "                pip_index = _index\n",
    "                break\n",
    "    dependencies[pip_index][\"pip\"] =  dependencies[pip_index][\"pip\"] + libraries\n",
    "    _conda_env[\"dependencies\"] = dependencies\n",
    "    return _conda_env\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Species\"]\n",
    "SPECIES = [\"Setosa\", \"Versicolor\", \"Virginica\"]\n",
    "\n",
    "\n",
    "def load_data(y_name=\"Species\"):\n",
    "    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split(\"/\")[-1], TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split(\"/\")[-1], TEST_URL)\n",
    "\n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features = dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.tensorflow.autolog()\n",
    "batch_size = 100\n",
    "train_steps = 1000\n",
    "runid = mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data\n",
    "(train_x, train_y), (test_x, test_y) = load_data()\n",
    "\n",
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpk_6blyoi\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpk_6blyoi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 23:40:53.870110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-13 23:40:53.870158: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-13 23:40:53.870196: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bneievrk-76946f66f-dtbkv): /proc/driver/nvidia/version does not exist\n",
      "2021-11-13 23:40:53.871087: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Two hidden layers of 10 nodes each.\n",
    "hidden_units = [10, 10]\n",
    "\n",
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=hidden_units,\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk_6blyoi/model.ckpt-1000\n",
      "WARNING:tensorflow:From /miniconda/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1078: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000...\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpk_6blyoi/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000...\n",
      "INFO:tensorflow:loss = 0.82140183, step = 1000\n",
      "INFO:tensorflow:global_step/sec: 55.685\n",
      "INFO:tensorflow:loss = 0.7917734, step = 1100 (2.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.0757\n",
      "INFO:tensorflow:loss = 0.7669014, step = 1200 (2.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.6867\n",
      "INFO:tensorflow:loss = 0.7893652, step = 1300 (2.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3216\n",
      "INFO:tensorflow:loss = 0.7778115, step = 1400 (2.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.0273\n",
      "INFO:tensorflow:loss = 0.70700437, step = 1500 (2.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.6114\n",
      "INFO:tensorflow:loss = 0.76223963, step = 1600 (2.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.18\n",
      "INFO:tensorflow:loss = 0.7604401, step = 1700 (2.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1611\n",
      "INFO:tensorflow:loss = 0.71477664, step = 1800 (2.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5684\n",
      "INFO:tensorflow:loss = 0.7514796, step = 1900 (2.125 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2000...\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmpk_6blyoi/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2000...\n",
      "INFO:tensorflow:Loss for final step: 0.76102316.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model.\n",
    "estimator = classifier.train(\n",
    "    input_fn=lambda: train_input_fn(train_x, train_y, batch_size),\n",
    "    steps=train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-13T23:42:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk_6blyoi/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 0.19000s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-13-23:42:07\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.73333335, average_loss = 0.7398704, global_step = 2000, loss = 0.7398704\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /tmp/tmpk_6blyoi/model.ckpt-2000\n",
      "\n",
      "Test set accuracy: 0.733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: eval_input_fn(test_x, test_y, batch_size)\n",
    ")\n",
    "\n",
    "print(\"\\nTest set accuracy: {accuracy:0.3f}\\n\".format(**eval_result))\n",
    "\n",
    "# Generate predictions from the model\n",
    "expected = [\"Setosa\", \"Versicolor\", \"Virginica\"]\n",
    "predict_x = {\n",
    "    \"SepalLength\": [5.1, 5.9, 6.9],\n",
    "    \"SepalWidth\": [3.3, 3.0, 3.1],\n",
    "    \"PetalLength\": [1.7, 4.2, 5.4],\n",
    "    \"PetalWidth\": [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda: eval_input_fn(predict_x, labels=None, batch_size=batch_size)\n",
    ")\n",
    "\n",
    "old_predictions = []\n",
    "template = '\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk_6blyoi/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Prediction is \"Setosa\" (33.7%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Virginica\" (50.6%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (66.1%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict[\"class_ids\"][0]\n",
    "    probability = pred_dict[\"probabilities\"][class_id]\n",
    "\n",
    "    print(template.format(SPECIES[class_id], 100 * probability, expec))\n",
    "\n",
    "    old_predictions.append(SPECIES[class_id])\n",
    "\n",
    "# Creating output tf.Variables to specify the output of the saved model.\n",
    "feat_specifications = {\n",
    "    \"SepalLength\": tf.Variable([], dtype=tf.float64, name=\"SepalLength\"),\n",
    "    \"SepalWidth\": tf.Variable([], dtype=tf.float64, name=\"SepalWidth\"),\n",
    "    \"PetalLength\": tf.Variable([], dtype=tf.float64, name=\"PetalLength\"),\n",
    "    \"PetalWidth\": tf.Variable([], dtype=tf.float64, name=\"PetalWidth\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/11/13 23:42:10 INFO mlflow.tensorflow: Logging TensorFlow Estimator as MLflow Model to run with ID '8a7036efeb454117bfbed4eec5a529db'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /miniconda/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float64>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(None,) dtype=float64>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(None,) dtype=float64>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(None,) dtype=float64>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float64>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(None,) dtype=float64>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(None,) dtype=float64>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(None,) dtype=float64>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpk_6blyoi/model.ckpt-2000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/tmppcfdjkni/temp-1636846930/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/11/13 23:42:11 INFO mlflow.tensorflow: Validating the specified TensorFlow model by attempting to load it in a new TensorFlow graph...\n",
      "2021-11-13 23:42:11.302002: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021/11/13 23:42:11 INFO mlflow.tensorflow: Validation succeeded!\n"
     ]
    },
    {
     "ename": "ResourceNotFoundError",
     "evalue": "The specified blob does not exist.\nRequestId:6155ab77-301e-0008-31e8-d84fb0000000\nTime:2021-11-13T23:42:16.4419763Z\nErrorCode:BlobNotFound",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1641/2662460579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# a MLflow Model), we don't need to use saved_estimator_path to load back the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# MLflow takes care of it!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpyfunc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifact_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_development\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredict_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_uri, suppress_warnings)\u001b[0m\n\u001b[1;32m    649\u001b[0m                               \u001b[0mmessages\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0memitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \"\"\"\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download_artifact_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m     \u001b[0mmodel_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLMODEL_FILE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/mlflow/tracking/artifact_utils.py\u001b[0m in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m    182\u001b[0m             )\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdownload_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_artifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_local_dir_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifact\u001b[0;34m(src_artifact_path, dst_local_dir_path)\u001b[0m\n\u001b[1;32m    128\u001b[0m             )\n\u001b[1;32m    129\u001b[0m             self._download_file(\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mremote_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_artifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_destination_file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             )\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlocal_destination_file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/mlflow/store/artifact/azure_blob_artifact_repo.py\u001b[0m in \u001b[0;36m_download_file\u001b[0;34m(self, remote_file_path, local_path)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mremote_full_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_root_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mcontainer_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_full_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/core/tracing/decorator.py\u001b[0m in \u001b[0;36mwrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mspan_impl_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspan_impl_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_container_client.py\u001b[0m in \u001b[0;36mdownload_blob\u001b[0;34m(self, blob, offset, length, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mblob_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_blob_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'merge_span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mblob_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     def _generate_delete_blobs_subrequest_options(\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/core/tracing/decorator.py\u001b[0m in \u001b[0;36mwrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mspan_impl_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspan_impl_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_blob_client.py\u001b[0m in \u001b[0;36mdownload_blob\u001b[0;34m(self, offset, length, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             **kwargs)\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mStorageStreamDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     def _quick_query_options(self, query_expression,\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_download.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, clients, config, start_range, end_range, validate_content, encryption_options, max_concurrency, name, container, encoding, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_download.py\u001b[0m in \u001b[0;36m_initial_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                     \u001b[0mprocess_storage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_shared/response_handlers.py\u001b[0m in \u001b[0;36mprocess_storage_error\u001b[0;34m(storage_error)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# `from None` prevents us from double printing the exception (suppresses generated layer error context)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raise error from None\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# pylint: disable=exec-used # nosec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_shared/response_handlers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_download.py\u001b[0m in \u001b[0;36m_initial_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0mdata_stream_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0mdownload_stream_current\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m                 )\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/storage/blob/_generated/operations/_blob_operations.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, snapshot, version_id, timeout, range, range_get_content_md5, range_get_content_crc64, request_id_parameter, lease_access_conditions, cpk_info, modified_access_conditions, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m206\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mmap_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailsafe_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStorageError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/azure/core/exceptions.py\u001b[0m in \u001b[0;36mmap_error\u001b[0;34m(status_code, response, error_map)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceNotFoundError\u001b[0m: The specified blob does not exist.\nRequestId:6155ab77-301e-0008-31e8-d84fb0000000\nTime:2021-11-13T23:42:16.4419763Z\nErrorCode:BlobNotFound"
     ]
    }
   ],
   "source": [
    "receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feat_specifications)\n",
    "temp = tempfile.mkdtemp()\n",
    "try:\n",
    "    # The model is automatically logged when export_saved_model() is called.\n",
    "    saved_estimator_path = classifier.export_saved_model(temp, receiver_fn).decode(\"utf-8\")\n",
    "    mlflow.end_run()\n",
    "    # Since the model was automatically logged as an artifact (more specifically\n",
    "    # a MLflow Model), we don't need to use saved_estimator_path to load back the model.\n",
    "    # MLflow takes care of it!\n",
    "    pyfunc_model = pyfunc.load_model(mlflow.get_artifact_uri(\"tf_development\"))\n",
    "\n",
    "    predict_data = [[5.1, 3.3, 1.7, 0.5], [5.9, 3.0, 4.2, 1.5], [6.9, 3.1, 5.4, 2.1]]\n",
    "    df = pd.DataFrame(\n",
    "        data=predict_data,\n",
    "        columns=[\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"],\n",
    "    )\n",
    "\n",
    "    # Predicting on the loaded Python Function and a DataFrame containing the\n",
    "    # original data we predicted on.\n",
    "    predict_df = pyfunc_model.predict(df)\n",
    "\n",
    "    # Checking the PyFunc's predictions are the same as the original model's predictions.\n",
    "    template = '\\nOriginal prediction is \"{}\", reloaded prediction is \"{}\"'\n",
    "    for expec, pred in zip(old_predictions, predict_df[\"classes\"]):\n",
    "        class_id = predict_df[\"class_ids\"][\n",
    "            predict_df.loc[predict_df[\"classes\"] == pred].index[0]\n",
    "        ]\n",
    "        reloaded_label = SPECIES[class_id]\n",
    "        print(template.format(expec, reloaded_label))\n",
    "finally:\n",
    "    shutil.rmtree(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying the model\n",
    "The above code logs a model in the experiments tab. For more info please refer [here](https://rocketml.gitbook.io/rocketml-user-guide/experiments). After deploying the model, we can obtain the model url for performing query as shown below.\n",
    "\n",
    "## 5. Query from the server\n",
    "\n",
    "There are two methods to perform query... The first is using `requests` library and the other using `curl` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:5011/invocations\"\n",
    "headers = {\"Content-Type\":\"text/csv\"}\n",
    "\n",
    "# First case, run inference on single data point\n",
    "np_array = np.random.rand(1,6).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "response = requests.post(url,data=json_data,headers=headers)\n",
    "if response.status_code == 200:\n",
    "    output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "    print(output)\n",
    "else:\n",
    "    print(response.status_code)\n",
    "    print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
    "\n",
    "# Second case, run inference on multiple data points\n",
    "np_array = np.random.rand(20,6).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "response = requests.post(url,data=json_data,headers=headers)\n",
    "if response.status_code == 200:\n",
    "    output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "    print(output)\n",
    "else:\n",
    "    print(response.status_code)\n",
    "    print(\"REST API deployment is in progress -- please try again in a few minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:5011/invocations -H 'Content-Type:text/csv' -d '[[0.6499166977064089, 0.17579454262114602, 0.2688911143313131, 0.7146591854799202, 0.6497433572112488, 0.7723469203958951]]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
