{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this workbook, we will train a simple Keras MNIST CNN model and deploy that for inference\n",
    "\n",
    "Parts of this workbook are borrowed from [here](https://keras.io/examples/vision/mnist_convnet/)\n",
    "\n",
    "## 2. Imports and Dependencies.\n",
    "The few packages needed are loaded next. Particularly, `numpy`, `tensorflow`, `keras`, `mlflow` will be majorly used in this tutorial. `requests` package will be used for performing query. `json` is used to post and get response from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 19:26:45.910967: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-14 19:26:45.911013: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import numpy as np\n",
    "from mlflow import pyfunc\n",
    "import cloudpickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLflow for experiment tracking and model deployment\n",
    "\n",
    "MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It tackles four primary functions:\n",
    "\n",
    "- Tracking experiments to record and compare parameters and results (MLflow Tracking).\n",
    "- Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\n",
    "- Providing a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\n",
    "\n",
    "More information [here](https://www.mlflow.org/docs/latest/index.html#)\n",
    "\n",
    "\n",
    "\n",
    "![image.png](https://www.mlflow.org/docs/latest/_images/scenario_4.png)\n",
    "\n",
    "- localhost maps to the server on which the current notebook is running\n",
    "\n",
    "- Tracking server maps to the server at environment variable `TRACKING_URL` that can be printed using `os.environ.get(\"TRACKING_URL\")`\n",
    "\n",
    "- Create an mlflow client that communicates with the tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import pyfunc\n",
    "\n",
    "# Setting a tracking uri to log the mlflow logs in a particular location tracked by \n",
    "from mlflow.tracking import MlflowClient\n",
    "tracking_uri = os.environ.get(\"TRACKING_URL\")\n",
    "client = MlflowClient(tracking_uri=tracking_uri)\n",
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create an experiment in mlflow database using mlflow client\n",
    "\n",
    "- Get the list of all the experiments (Click on **Experiments** tab on the sidebar to see the list)\n",
    "- Create a new experiment named *numpy_deployment* if it doesn't exist\n",
    "- Set *numpy_deployment* as the new experiment under which different **runs** are tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MLflow Entity Hierarchy\n",
    "\n",
    "- Experiment 1\n",
    "    - Run 1\n",
    "        - Parameters\n",
    "        - Metrics\n",
    "        - Artifacts\n",
    "            - Folder 1\n",
    "                - File 1\n",
    "                - File 2\n",
    "            - Folder 2 \n",
    "    - Run 2\n",
    "    - Run 3\n",
    "\n",
    "- Experiment 2\n",
    "- Experiment 3        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a tracking project experiment name to keep the experiments organized\n",
    "experiments = client.list_experiments()\n",
    "experiment_names = []\n",
    "for exp in experiments:\n",
    "    experiment_names.append(exp.name)\n",
    "experiment_name = \"keras_deployment\"\n",
    "if experiment_name not in experiment_names:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python Class for inference\n",
    "\n",
    "- ModelWrapper is derived from mlflow.pyfunc.PythonModel [more info](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html)\n",
    "- load_context() member function is used to load the model. In this case, it loads a keras trained model which can be loaded.\n",
    "- predict member function takes a numpy array as input and outputs another numpy array\n",
    "- An object of this class will be saved as a pickle file in blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Wrapper that takes \n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self,context):\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        self.model = tf.keras.models.load_model(context.artifacts['model_path'])\n",
    "        print(\"Model initialized\")\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        import numpy as np\n",
    "        import json\n",
    "        import tensorflow as tf\n",
    "        json_txt = \", \".join(model_input.columns)\n",
    "        data_list = json.loads(json_txt)\n",
    "        inputs = np.array(data_list)\n",
    "        print(inputs.shape)\n",
    "        if len(inputs.shape) == 4:\n",
    "            print('batch inference')\n",
    "            predictions = self.model.predict(inputs)\n",
    "            predictions = predictions.tolist()\n",
    "        elif len(inputs.shape) == 3:\n",
    "            print('single inference')\n",
    "            predictions = self.model.predict(np.expand_dims(inputs,0))\n",
    "            predictions = predictions.tolist()\n",
    "        else:\n",
    "            raise ValueError('invalid input shape')\n",
    "        return json.dumps(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Register a model using mlflow\n",
    "\n",
    "- Log user-defined parameters in a remote database through a remote server\n",
    "- Create a model_wrapper object using ModelWrapper() class in the above cell\n",
    "- Create a default conda environment that need to be installed on the Docker conatiner that serves a REST API\n",
    "- Save the model object as a pickle file and conda environment as artifacts (files) in S3 or Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.Training\n",
    "\n",
    "We download the MNIST dataset using utilities. MNIST dataset contains hand written digits. mlflow can automatically log all the metrics along with model. Once the training is complete, mlflow can log the model that needs to be used for inference.\n",
    "\n",
    "First, we download the dataset and perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a CNN model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 19:26:48.571004: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-14 19:26:48.571058: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-14 19:26:48.571080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rlxlgt66-b87c796c-vfrc5): /proc/driver/nvidia/version does not exist\n",
      "2021-11-14 19:26:48.571417: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compiling the model and performing the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 19:26:48.771078: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.3714 - accuracy: 0.8880 - val_loss: 0.0868 - val_accuracy: 0.9757\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 19s 46ms/step - loss: 0.1128 - accuracy: 0.9652 - val_loss: 0.0642 - val_accuracy: 0.9828\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 19s 46ms/step - loss: 0.0857 - accuracy: 0.9734 - val_loss: 0.0482 - val_accuracy: 0.9873\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 19s 45ms/step - loss: 0.0733 - accuracy: 0.9776 - val_loss: 0.0437 - val_accuracy: 0.9888\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 19s 46ms/step - loss: 0.0646 - accuracy: 0.9798 - val_loss: 0.0402 - val_accuracy: 0.9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 19:28:26.467713: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./keras-model/assets\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate the python inference model wrapper for the server\n",
    "model_wrapper = ModelWrapper()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# checkpointing and logging the model in mlflow\n",
    "artifact_path = './keras-model'\n",
    "model.save(artifact_path)\n",
    "model_artifacts = {\"model_path\" : artifact_path}\n",
    "env = mlflow.tensorflow.get_default_conda_env()\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_summary\",model.summary())\n",
    "    mlflow.log_param(\"epochs\",epochs)\n",
    "    mlflow.log_param(\"batch_size\",batch_size)\n",
    "    mlflow.log_param(\"training_history\",history)\n",
    "    mlflow.pyfunc.log_model(\"keras_model\", python_model=model_wrapper, artifacts=model_artifacts, conda_env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying the model\n",
    "The above code logs a model in the experiments tab. For more info please refer [here](https://rocketml.gitbook.io/rocketml-user-guide/experiments). \n",
    "\n",
    "### 4.1 Find experiment in experiment list and click on it\n",
    "![experiments_list](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/experiments_list.png)\n",
    "\n",
    "### 4.2 Find run in runs list and click on it\n",
    "![runs_list](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/runs_list.png)\n",
    "\n",
    "### 4.3 Get run details and click on artifacts\n",
    "![run_details](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/run_details.png)\n",
    "\n",
    "### 4.4 Check different files logged as artifacts\n",
    "![artifacts](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/artifacts.png)\n",
    "\n",
    "- An MLflow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools [More Details](https://www.mlflow.org/docs/latest/models.html#storage-format)\n",
    "- ModelWrapper() object is saved as pkl file\n",
    "- conda.yaml and requirements.txt file are used to manage Python environment\n",
    "- Numpy file is saved in artifacts folder within the main folder (np_model)\n",
    "\n",
    "### 4.5 Deploy ML model as a REST API service\n",
    "\n",
    "Click on **Convert To Model** and fill the form. **Note: For deploying the Keras model, please select at least 4096MB for max memory!**\n",
    "\n",
    "![model_deployment](https://github.com/rocketmlhq/sciml/raw/cd1455b1e3bc09e5d0e847af87bd5ac1775add88/03_Deployment/model_deployment_keras_4GB.png)\n",
    "\n",
    "### 4.6 Go to models tab and wait until the model turns to **ON** state\n",
    "![model_list](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/model_list.png)\n",
    "\n",
    "## 5. Use the Endpoint and Query from the server\n",
    "\n",
    "There are two methods to perform query... The first is using `requests` library and the other using `curl` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.4550702e-03 4.0400081e-04 3.8316324e-02 2.7348047e-02 4.0564043e-03\n",
      "  7.5183250e-02 7.6675541e-03 5.5649033e-04 8.3789396e-01 3.1188931e-03]]\n",
      "[[2.9919872e-03 1.2010236e-03 7.7142179e-02 3.4051750e-02 1.2698015e-02\n",
      "  9.8876357e-02 2.0492699e-03 1.7935239e-02 7.4503058e-01 8.0236373e-03]\n",
      " [9.3202889e-03 2.1819610e-04 4.3830331e-02 3.8132112e-02 5.9867408e-03\n",
      "  1.1520740e-01 3.1106498e-03 5.8234786e-04 7.7803266e-01 5.5792583e-03]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "################################################################################\n",
    "# *** SET MODEL URL HERE BEFORE RUNNING THIS CELL (instructions above) ***\n",
    "# Example: https://<random_string>.sciml.rocketml.net/invocations\n",
    "url = \"\"\n",
    "################################################################################\n",
    "\n",
    "if not url:\n",
    "    raise ValueError('Model URL not set! Please read instructions on how to deploy model, set the correct URL, and try again.')\n",
    "\n",
    "headers = {\"Content-Type\":\"text/csv\"}\n",
    "\n",
    "# First case, run inference on single data point\n",
    "np_array = np.random.rand(28,28,1).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "\n",
    "if url:\n",
    "    response = requests.post(url,data=json_data,headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "        print(output)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
    "else:\n",
    "    print(\"Make sure that the model is in ON state. Copy the Endpoint\")\n",
    "\n",
    "# Second case, run inference on multiple data points\n",
    "np_array = np.random.rand(2,28,28,1).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "\n",
    "if url:\n",
    "    response = requests.post(url,data=json_data,headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "        print(output)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
    "else:\n",
    "    print(\"Make sure that the model is in ON state. Copy the Endpoint\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
