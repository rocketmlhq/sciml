{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment using Numpy Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this workbook, we will look into the basics of deploying a model. For simplicity, we will consider a simple numpy linear classifier $$ \\mathbf{Y} = \\mathbf{W} \\mathbf{X} + \\mathbf{b}$$\n",
    "\n",
    "For simplicity, we will consider $\\mathbf{X}$ to be 6 dimensional ($\\mathbb{R}^6$). i.e. 1 data point $x \\in \\mathbf{X}$ will be a numpy array of shape $(1,6)$. The output $\\mathbf{Y}$ is 3 dimensional ($\\mathbb{R}^3$). Then, the weights $\\mathbf{W}$ will be a numpy array of shape $(3,6)$ and bias $\\mathbf{b}$ will be a numpy array of shape $(,3)$. \n",
    "\n",
    "In this workbook, we will demonstrate how to deploy this numpy linear classifier as a server and how to perform query on this numpy linear classifier.\n",
    "\n",
    "## 2. Imports and Dependencies.\n",
    "The few packages needed are loaded next. Particularly, `numpy`, `mlflow` will be majorly used in this tutorial. `requests` package will be used for performing query. `json` is used to post and get response from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow for experiment tracking and model deployment\n",
    "\n",
    "MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It tackles four primary functions:\n",
    "\n",
    "- Tracking experiments to record and compare parameters and results (MLflow Tracking).\n",
    "- Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\n",
    "- Providing a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\n",
    "\n",
    "More information [here](https://www.mlflow.org/docs/latest/index.html#)\n",
    "\n",
    "\n",
    "\n",
    "![image.png](https://www.mlflow.org/docs/latest/_images/scenario_4.png)\n",
    "\n",
    "- localhost maps to the server on which the current notebook is running\n",
    "\n",
    "- Tracking server maps to the server at environment variable `TRACKING_URL` that can be printed using `os.environ.get(\"TRACKING_URL\")`\n",
    "\n",
    "- Create an mlflow client that communicates with the tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import pyfunc\n",
    "\n",
    "# Setting a tracking uri to log the mlflow logs in a particular location tracked by \n",
    "from mlflow.tracking import MlflowClient\n",
    "tracking_uri = os.environ.get(\"TRACKING_URL\")\n",
    "client = MlflowClient(tracking_uri=tracking_uri)\n",
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment in mlflow database using mlflow client\n",
    "\n",
    "- Get the list of all the experiments (Click on **Experiments** tab on the sidebar to see the list)\n",
    "- Create a new experiment named *numpy_deployment* if it doesn't exist\n",
    "- Set *numpy_deployment* as the new experiment under which different **runs** are tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Entity Hierarchy\n",
    "\n",
    "- Experiment 1\n",
    "    - Run 1\n",
    "        - Parameters\n",
    "        - Metrics\n",
    "        - Artifacts\n",
    "            - Folder 1\n",
    "                - File 1\n",
    "                - File 2\n",
    "            - Folder 2 \n",
    "    - Run 2\n",
    "    - Run 3\n",
    "\n",
    "- Experiment 2\n",
    "- Experiment 3        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a tracking project experiment name to keep the experiments organized\n",
    "experiments = client.list_experiments()\n",
    "experiment_names = []\n",
    "for exp in experiments:\n",
    "    experiment_names.append(exp.name)\n",
    "experiment_name = \"numpy_deployment\"\n",
    "if experiment_name not in experiment_names:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Class for inference\n",
    "\n",
    "- ModelWrapper is derived from mlflow.pyfunc.PythonModel [more info](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html)\n",
    "- load_context() member function is used to load the model. In this case, it loads a numpy file with two arrays **weights** and **bias**\n",
    "- predict member function takes a numpy array as input and outputs another numpy array\n",
    "- An object of this class will be saved as a pickle file in blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Wrapper that takes \n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self,context):\n",
    "        import numpy as np\n",
    "        self.model = np.load(context.artifacts['model_path'], allow_pickle=True).tolist()\n",
    "        print(\"Model initialized\")\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        import numpy as np\n",
    "        import json\n",
    "        json_txt = \", \".join(model_input.columns)\n",
    "        data_list = json.loads(json_txt)\n",
    "        inputs = np.array(data_list)\n",
    "        if len(inputs.shape) == 2:\n",
    "            print('batch inference')\n",
    "            predictions = []\n",
    "            for idx in range(inputs.shape[0]):\n",
    "                prediction = np.matmul(inputs[idx,:],self.model['weights'].T) + self.model['bias']\n",
    "                predictions.append(prediction.tolist())\n",
    "        elif len(inputs.shape) == 1:\n",
    "            print('single inference')\n",
    "            predictions = self.model['weights'].T * inputs + self.model['bias']\n",
    "            predictions = predictions.tolist()\n",
    "        else:\n",
    "            raise ValueError('invalid input shape')\n",
    "        return json.dumps(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a model using mlflow\n",
    "\n",
    "- Log user-defined parameters in a remote database through a remote server\n",
    "- Create a model_wrapper object using ModelWrapper() class in the above cell\n",
    "- Create a default conda environment that need to be installed on the Docker conatiner that serves a REST API\n",
    "- Save the model object as a pickle file and conda environment as artifacts (files) in S3 or Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the python inference model wrapper for the server\n",
    "model_wrapper = ModelWrapper()\n",
    "\n",
    "\n",
    "# define the model weights randomly\n",
    "np_weights = np.random.rand(3,6)\n",
    "np_bias = np.random.rand(3)\n",
    "\n",
    "# checkpointing and logging the model in mlflow\n",
    "artifact_path = './np_model'\n",
    "np.save(artifact_path, {'weights':np_weights, 'bias':np_bias})\n",
    "model_artifacts = {\"model_path\" : artifact_path+'.npy'}\n",
    "\n",
    "#Conda environment\n",
    "env = mlflow.sklearn.get_default_conda_env()\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"features\",6)\n",
    "    mlflow.log_param(\"labels\",3)\n",
    "    mlflow.pyfunc.log_model(\"np_model\", python_model=model_wrapper, artifacts=model_artifacts, conda_env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying the model\n",
    "The above code logs a model in the experiments tab. For more info please refer [here](https://rocketml.gitbook.io/rocketml-user-guide/experiments). \n",
    "\n",
    "### 4.1 Find experiment in experiment list and click on it\n",
    "![experiments_list](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/experiments_list.png)\n",
    "\n",
    "### 4.2 Find run in runs list and click on it\n",
    "![runs_list](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/runs_list.png)\n",
    "\n",
    "### 4.3 Get run details and click on artifacts\n",
    "![run_details](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/run_details.png)\n",
    "\n",
    "### 4.4 Check different files logged as artifacts\n",
    "![artifacts](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/artifacts.png)\n",
    "\n",
    "- An MLflow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools [More Details](https://www.mlflow.org/docs/latest/models.html#storage-format)\n",
    "- ModelWrapper() object is saved as pkl file\n",
    "- conda.yaml and requirements.txt file are used to manage Python environment\n",
    "- Numpy file is saved in artifacts folder within the main folder (np_model)\n",
    "\n",
    "### 4.5 Deploy ML model as a REST API service\n",
    "\n",
    "Click on **Convert To Model** and fill the form\n",
    "\n",
    "![model_deployment](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/model_deployment.png)\n",
    "\n",
    "### 4.6 Go to models tab and wait until the model turns to **ON** state\n",
    "![model_list](https://github.com/rocketmlhq/sciml/raw/e8abbef269c5bee9d2b69398495fc5ced7457708/03_Deployment/model_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use the Endpoint and Query from the server\n",
    "\n",
    "There are two methods to perform query... The first is using `requests` library and the other using `curl` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "################################################################################\n",
    "# *** SET MODEL URL HERE BEFORE RUNNING THIS CELL (instructions above) ***\n",
    "# Example: https://<random_string>.sciml.rocketml.net/invocations\n",
    "url = \"\"\n",
    "################################################################################\n",
    "\n",
    "if not url:\n",
    "    raise ValueError('Model URL not set! Please read instructions on how to deploy model, set the correct URL, and try again.')\n",
    "\n",
    "headers = {\"Content-Type\":\"text/csv\"}\n",
    "\n",
    "# First case, run inference on single data point\n",
    "np_array = np.random.rand(1,6).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "\n",
    "if url:\n",
    "    response = requests.post(url,data=json_data,headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "        print(output)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
    "else:\n",
    "    print(\"Make sure that the model is in ON state. Copy the Endpoint\")\n",
    "\n",
    "# Second case, run inference on multiple data points\n",
    "np_array = np.random.rand(20,6).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "\n",
    "if url:\n",
    "    response = requests.post(url,data=json_data,headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "        print(output)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
    "else:\n",
    "    print(\"Make sure that the model is in ON state. Copy the Endpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
