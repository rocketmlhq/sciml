{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffNet Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will show  how to use DiffNets for 2D and 3D Poisson's equattion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: installing dependencies and importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    # 'font.family': 'serif',\n",
    "    'font.size':12,\n",
    "})\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "seed_everything(42)\n",
    "\n",
    "import DiffNet\n",
    "from DiffNet.networks.wgan_old import GoodGenerator\n",
    "from DiffNet.networks.autoencoders import AE\n",
    "from DiffNet.DiffNetFEM import DiffNet2DFEM\n",
    "from DiffNet.datasets.parametric.klsum import KLSumStochastic\n",
    "from DiffNet.datasets.single_instances.klsum import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define few classes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils import data\n",
    "from DiffNet.gen_input_calc import generate_diffusivity_tensor\n",
    "\n",
    "\n",
    "class KLSum(data.Dataset):\n",
    "    'PyTorch dataset for sampling coefficients'\n",
    "    def __init__(self, filename, domain_size=64, kl_terms=6):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \"\"\"\n",
    "        self.coeffs = np.load(filename)\n",
    "        self.domain_size = domain_size\n",
    "        self.kl_terms = kl_terms\n",
    "        self.dataset = []\n",
    "        \n",
    "        print('loading dataset')\n",
    "        for coeff in tqdm(self.coeffs[:100]):\n",
    "            domain = generate_diffusivity_tensor(coeff, output_size=self.domain_size, n_sum_nu=kl_terms).squeeze()\n",
    "            # bc1 will be source, u will be set to 1 at these locations\n",
    "            bc1 = np.zeros_like(domain)\n",
    "            bc1[:,0] = 1\n",
    "\n",
    "            # bc2 will be sink, u will be set to 0 at these locations\n",
    "            bc2 = np.zeros_like(domain)\n",
    "            bc2[:,-1] = 1\n",
    "\n",
    "            self.dataset.append(np.array([domain,bc1,bc2]))\n",
    "        self.dataset = np.array(self.dataset)\n",
    "        self.n_samples = self.dataset.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        inputs = self.dataset[index]\n",
    "        forcing = np.zeros_like(self.dataset[index][0])\n",
    "        return torch.FloatTensor(inputs), torch.FloatTensor(forcing).unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poisson(DiffNet2DFEM):\n",
    "    \"\"\"docstring for Poisson\"\"\"\n",
    "    def __init__(self, network, dataset, **kwargs):\n",
    "        super(Poisson, self).__init__(network, dataset, **kwargs)\n",
    "\n",
    "    def loss(self, u, inputs_tensor, forcing_tensor):\n",
    "\n",
    "        f = forcing_tensor # renaming variable\n",
    "        \n",
    "        # extract diffusivity and boundary conditions here\n",
    "        nu = inputs_tensor[:,0:1,:,:]\n",
    "        bc1 = inputs_tensor[:,1:2,:,:]\n",
    "        bc2 = inputs_tensor[:,2:3,:,:]\n",
    "\n",
    "        # apply boundary conditions\n",
    "        u = torch.where(bc1>0.5,1.0+u*0.0,u)\n",
    "        u = torch.where(bc2>0.5,u*0.0,u)\n",
    "\n",
    "\n",
    "        nu_gp = self.gauss_pt_evaluation(nu)\n",
    "        f_gp = self.gauss_pt_evaluation(f)\n",
    "        u_gp = self.gauss_pt_evaluation(u)\n",
    "        u_x_gp = self.gauss_pt_evaluation_der_x(u)\n",
    "        u_y_gp = self.gauss_pt_evaluation_der_y(u)\n",
    "\n",
    "        transformation_jacobian = self.gpw.unsqueeze(-1).unsqueeze(-1).unsqueeze(0).type_as(nu_gp)\n",
    "        res_elmwise = transformation_jacobian * (nu_gp * (u_x_gp**2 + u_y_gp**2) - (u_gp * f_gp))\n",
    "        res_elmwise = torch.sum(res_elmwise, 1) \n",
    "\n",
    "        # transformation_jacobian = (0.5 * self.h)**2 * self.gpw.unsqueeze(-1).unsqueeze(-1).unsqueeze(0).type_as(nu_gp)\n",
    "        # res_elmwise = 0.5 * transformation_jacobian * (nu_gp * (u_x_gp**2 + u_y_gp**2) - (u_gp * f_gp))\n",
    "        # res_elmwise = torch.sum(res_elmwise, 1) \n",
    "\n",
    "        loss = torch.mean(res_elmwise)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inputs_tensor, forcing_tensor = batch\n",
    "        u = self.network(inputs_tensor[:,0:1,:,:])\n",
    "        return u, inputs_tensor, forcing_tensor\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        u, inputs_tensor, forcing_tensor = self.forward(batch)\n",
    "        loss_val = self.loss(u, inputs_tensor, forcing_tensor).mean()\n",
    "        return {\"loss\": loss_val}\n",
    "\n",
    "    def training_step_end(self, training_step_outputs):\n",
    "        loss = training_step_outputs[\"loss\"]\n",
    "        self.log('PDE_loss', loss.item())\n",
    "        self.log('loss', loss.item())\n",
    "        return training_step_outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        # opts = [torch.optim.LBFGS(self.network.parameters(), lr=lr, max_iter=5)]\n",
    "        opts = [torch.optim.Adam(self.network.parameters(), lr=lr)]\n",
    "        # schd = []\n",
    "        schd = [torch.optim.lr_scheduler.MultiStepLR(opts[0], milestones=[10,15,30], gamma=0.1)]\n",
    "        return opts, schd\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        num_query = 6\n",
    "        plt_num_row = num_query\n",
    "        plt_num_col = 2\n",
    "        fig, axs = plt.subplots(plt_num_row, plt_num_col, figsize=(2*plt_num_col,1.2*plt_num_row),\n",
    "                            subplot_kw={'aspect': 'auto'}, sharex=True, sharey=True, squeeze=True)\n",
    "        for ax_row in axs:\n",
    "            for ax in ax_row:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "        \n",
    "        self.network.eval()\n",
    "        inputs, forcing = self.dataset[0:num_query]\n",
    "        forcing = forcing.repeat(num_query,1,1,1)\n",
    "\n",
    "        ub, inputs_tensor, forcing_tensor = self.forward((inputs.type_as(next(self.network.parameters())), forcing.type_as(next(self.network.parameters()))))\n",
    "        \n",
    "        loss = self.loss(ub, inputs_tensor, forcing_tensor[:,0:1,:,:])\n",
    "\n",
    "        for idx in range(num_query):\n",
    "            f = forcing_tensor # renaming variable\n",
    "            \n",
    "            # extract diffusivity and boundary conditions here\n",
    "            nu = inputs_tensor[idx,0:1,:,:]\n",
    "            u = ub[idx,0:1,:,:]\n",
    "            bc1 = inputs_tensor[idx,1:2,:,:]\n",
    "            bc2 = inputs_tensor[idx,2:3,:,:]\n",
    "\n",
    "            # apply boundary conditions\n",
    "            u = torch.where(bc1>0.5,1.0+u*0.0,u)\n",
    "            u = torch.where(bc2>0.5,u*0.0,u)\n",
    "\n",
    "            k = nu.squeeze().detach().cpu()\n",
    "            u = u.squeeze().detach().cpu()\n",
    "\n",
    "            im0 = axs[idx][0].imshow(k,cmap='jet')\n",
    "            fig.colorbar(im0, ax=axs[idx,0])\n",
    "            im1 = axs[idx][1].imshow(u,cmap='jet')\n",
    "            fig.colorbar(im1, ax=axs[idx,1])  \n",
    "        plt.savefig(os.path.join(self.logger[0].log_dir, 'contour_' + str(self.current_epoch) + '.png'))\n",
    "        self.logger[0].experiment.add_figure('Contour Plots', fig, self.current_epoch)\n",
    "        plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-12 21:50:53--  https://github.com/rocketmlhq/sciml/raw/main/05_DiffNets/sobol_6d.npy\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/rocketmlhq/sciml/main/05_DiffNets/sobol_6d.npy [following]\n",
      "--2021-11-12 21:50:53--  https://raw.githubusercontent.com/rocketmlhq/sciml/main/05_DiffNets/sobol_6d.npy\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3145856 (3.0M) [application/octet-stream]\n",
      "Saving to: ‘sobol_6d.npy.1’\n",
      "\n",
      "sobol_6d.npy.1      100%[===================>]   3.00M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2021-11-12 21:50:54 (45.1 MB/s) - ‘sobol_6d.npy.1’ saved [3145856/3145856]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/rocketmlhq/sciml/raw/main/05_DiffNets/sobol_6d.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_epochs =  10\n",
      "loading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1189.08it/s]\n",
      "Missing logger folder: ./klsum_32\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "kl_terms = 6\n",
    "domain_size = 32\n",
    "LR = 1e-3\n",
    "batch_size = 128\n",
    "sample_size = 65536\n",
    "sobol_file = 'sobol_'+str(kl_terms)+'d.npy'\n",
    "max_epochs = 10\n",
    "print(\"Max_epochs = \", max_epochs)\n",
    "\n",
    "dataset = KLSum(sobol_file, domain_size=domain_size, kl_terms=kl_terms)\n",
    "# dataset = Dataset('../single_instance/example-coefficients.txt', domain_size=64)\n",
    "network = AE(in_channels=1, out_channels=1, dims=16, n_downsample=2)\n",
    "basecase = Poisson(network, dataset, batch_size=batch_size, domain_size=domain_size, learning_rate=LR)\n",
    "\n",
    "# ------------------------\n",
    "# 1 INIT TRAINER\n",
    "# ------------------------\n",
    "logger = pl.loggers.TensorBoardLogger('.', name=\"klsum_\"+str(domain_size))\n",
    "csv_logger = pl.loggers.CSVLogger(logger.save_dir, name=logger.name, version=logger.version)\n",
    "\n",
    "checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(monitor='loss',\n",
    "    dirpath=logger.log_dir, filename='{epoch}-{step}',\n",
    "    mode='min', save_last=True)\n",
    "\n",
    "trainer = Trainer(callbacks=[checkpoint],\n",
    "    checkpoint_callback=True, logger=[logger,csv_logger],\n",
    "    max_epochs=max_epochs, deterministic=True, profiler='simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 21:51:10.125465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-12 21:51:10.125520: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/loggers/csv_logs.py:58: UserWarning: Experiment logs directory ./klsum_32/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  f\"Experiment logs directory {self.log_dir} exists and is not empty.\"\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | network   | AE            | 264 K \n",
      "1 | N_gp      | ParameterList | 16    \n",
      "2 | dN_x_gp   | ParameterList | 16    \n",
      "3 | dN_y_gp   | ParameterList | 16    \n",
      "4 | d2N_x_gp  | ParameterList | 16    \n",
      "5 | d2N_y_gp  | ParameterList | 16    \n",
      "6 | d2N_xy_gp | ParameterList | 16    \n",
      "--------------------------------------------\n",
      "264 K     Trainable params\n",
      "96        Non-trainable params\n",
      "264 K     Total params\n",
      "1.058     Total estimated model params size (MB)\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/ubuntu/efs/sciml/05_DiffNets/klsum_32/version_0 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:395: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:03<00:00,  3.97s/it, loss=5.57e+04, v_num=0_0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  53.917         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  3.6736         \t|10             \t|  36.736         \t|  68.135         \t|\n",
      "run_training_batch                 \t|  2.1149         \t|10             \t|  21.149         \t|  39.225         \t|\n",
      "optimizer_step_with_closure_0      \t|  2.1144         \t|10             \t|  21.144         \t|  39.215         \t|\n",
      "training_step_and_backward         \t|  2.1123         \t|10             \t|  21.123         \t|  39.177         \t|\n",
      "backward                           \t|  1.5393         \t|10             \t|  15.393         \t|  28.549         \t|\n",
      "on_epoch_end                       \t|  1.2614         \t|10             \t|  12.614         \t|  23.396         \t|\n",
      "model_forward                      \t|  0.55471        \t|10             \t|  5.5471         \t|  10.288         \t|\n",
      "training_step                      \t|  0.54516        \t|10             \t|  5.4516         \t|  10.111         \t|\n",
      "on_train_epoch_end                 \t|  0.25252        \t|10             \t|  2.5252         \t|  4.6835         \t|\n",
      "zero_grad                          \t|  0.018266       \t|10             \t|  0.18266        \t|  0.33878        \t|\n",
      "on_train_epoch_start               \t|  0.017313       \t|10             \t|  0.17313        \t|  0.3211         \t|\n",
      "on_train_end                       \t|  0.13392        \t|1              \t|  0.13392        \t|  0.24839        \t|\n",
      "on_train_batch_end                 \t|  0.011089       \t|10             \t|  0.11089        \t|  0.20568        \t|\n",
      "training_step_end                  \t|  0.0093381      \t|10             \t|  0.093381       \t|  0.17319        \t|\n",
      "get_train_batch                    \t|  0.0024218      \t|20             \t|  0.048436       \t|  0.089835       \t|\n",
      "fetch_next_train_batch             \t|  0.0023956      \t|20             \t|  0.047913       \t|  0.088864       \t|\n",
      "on_pretrain_routine_start          \t|  0.014876       \t|1              \t|  0.014876       \t|  0.02759        \t|\n",
      "on_train_batch_start               \t|  0.0002523      \t|10             \t|  0.002523       \t|  0.0046795      \t|\n",
      "on_train_start                     \t|  0.0017394      \t|1              \t|  0.0017394      \t|  0.0032261      \t|\n",
      "training_batch_to_device           \t|  0.00013549     \t|10             \t|  0.0013549      \t|  0.0025129      \t|\n",
      "configure_optimizers               \t|  0.00069674     \t|1              \t|  0.00069674     \t|  0.0012923      \t|\n",
      "on_epoch_start                     \t|  6.4983e-05     \t|10             \t|  0.00064983     \t|  0.0012053      \t|\n",
      "on_after_backward                  \t|  4.2292e-05     \t|10             \t|  0.00042292     \t|  0.0007844      \t|\n",
      "on_before_backward                 \t|  2.9952e-05     \t|10             \t|  0.00029952     \t|  0.00055552     \t|\n",
      "on_batch_end                       \t|  2.8241e-05     \t|10             \t|  0.00028241     \t|  0.0005238      \t|\n",
      "on_before_zero_grad                \t|  2.7851e-05     \t|10             \t|  0.00027851     \t|  0.00051657     \t|\n",
      "on_batch_start                     \t|  2.6651e-05     \t|10             \t|  0.00026651     \t|  0.00049431     \t|\n",
      "on_before_optimizer_step           \t|  2.3501e-05     \t|10             \t|  0.00023501     \t|  0.00043588     \t|\n",
      "train_dataloader                   \t|  0.00014161     \t|1              \t|  0.00014161     \t|  0.00026264     \t|\n",
      "on_before_accelerator_backend_setup\t|  3.0702e-05     \t|1              \t|  3.0702e-05     \t|  5.6944e-05     \t|\n",
      "on_fit_end                         \t|  2.9802e-05     \t|1              \t|  2.9802e-05     \t|  5.5274e-05     \t|\n",
      "on_configure_sharded_model         \t|  2.6901e-05     \t|1              \t|  2.6901e-05     \t|  4.9894e-05     \t|\n",
      "on_fit_start                       \t|  2.5201e-05     \t|1              \t|  2.5201e-05     \t|  4.6741e-05     \t|\n",
      "setup                              \t|  2.5101e-05     \t|1              \t|  2.5101e-05     \t|  4.6555e-05     \t|\n",
      "teardown                           \t|  2.1201e-05     \t|1              \t|  2.1201e-05     \t|  3.9322e-05     \t|\n",
      "on_pretrain_routine_end            \t|  1.9501e-05     \t|1              \t|  1.9501e-05     \t|  3.6169e-05     \t|\n",
      "prepare_data                       \t|  1.2301e-05     \t|1              \t|  1.2301e-05     \t|  2.2815e-05     \t|\n",
      "configure_sharded_model            \t|  1.1001e-05     \t|1              \t|  1.1001e-05     \t|  2.0404e-05     \t|\n",
      "on_train_dataloader                \t|  6.201e-06      \t|1              \t|  6.201e-06      \t|  1.1501e-05     \t|\n",
      "configure_callbacks                \t|  6.2e-06        \t|1              \t|  6.2e-06        \t|  1.1499e-05     \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 4 Training\n",
    "# ------------------------\n",
    "trainer.fit(basecase)\n",
    "\n",
    "# ------------------------\n",
    "# 5 SAVE NETWORK\n",
    "# ------------------------\n",
    "torch.save(basecase.network, os.path.join(logger.log_dir, 'network.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
