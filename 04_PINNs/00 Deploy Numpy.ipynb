{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment using Numpy Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this workbook, we will look into the basics of deploying a model. For simplicity, we will consider a simple numpy linear classifier $$ \\mathbf{Y} = \\mathbf{W} \\mathbf{X} + \\mathbf{b}$$\n",
    "\n",
    "For simplicity, we will consider $\\mathbf{X}$ to be 6 dimensional ($\\mathbb{R}^6$). i.e. 1 data point $x \\in \\mathbf{X}$ will be a numpy array of shape $(1,6)$. The output $\\mathbf{Y}$ is 3 dimensional ($\\mathbb{R}^3$). Then, the weights $\\mathbf{W}$ will be a numpy array of shape $(3,6)$ and bias $\\mathbf{b}$ will be a numpy array of shape $(,3)$. \n",
    "\n",
    "In this workbook, we will demonstrate how to deploy this numpy linear classifier as a server and how to perform query on this numpy linear classifier.\n",
    "\n",
    "## 2. Imports and Dependencies.\n",
    "The few packages needed are loaded next. Particularly, `numpy`, `mlflow` will be majorly used in this tutorial. `requests` package will be used for performing query. `json` is used to post and get response from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from mlflow import pyfunc\n",
    "\n",
    "# Setting a tracking uri to log the mlflow logs in a particular location tracked by \n",
    "from mlflow.tracking import MlflowClient\n",
    "tracking_uri = os.environ.get(\"TRACKING_URL\")\n",
    "client = MlflowClient(tracking_uri=tracking_uri)\n",
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility function to add libraries to conda environment\n",
    "def add_libraries_to_conda_env(_conda_env,libraries=[],conda_dependencies=[]):\n",
    "    dependencies = _conda_env[\"dependencies\"]\n",
    "    dependencies = dependencies + conda_dependencies\n",
    "    pip_index = None\n",
    "    for _index,_element in enumerate(dependencies):\n",
    "        if type(_element) == dict:\n",
    "            if \"pip\" in _element.keys():\n",
    "                pip_index = _index\n",
    "                break\n",
    "    dependencies[pip_index][\"pip\"] =  dependencies[pip_index][\"pip\"] + libraries\n",
    "    _conda_env[\"dependencies\"] = dependencies\n",
    "    return _conda_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Wrapper that takes X as input using json and predicts an output Y\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self,context):\n",
    "        import numpy as np\n",
    "        self.model = np.load(context.artifacts['model_path'], allow_pickle=True).tolist()\n",
    "        print(\"Model initialized\")\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        import numpy as np\n",
    "        import json\n",
    "        json_txt = \", \".join(model_input.columns)\n",
    "        data_list = json.loads(json_txt)\n",
    "        inputs = np.array(data_list)\n",
    "        if len(inputs.shape) == 2:\n",
    "            print('batch inference')\n",
    "            predictions = []\n",
    "            for idx in range(inputs.shape[0]):\n",
    "                prediction = np.matmul(inputs[idx,:],self.model['weights'].T) + self.model['bias']\n",
    "                predictions.append(prediction.tolist())\n",
    "        elif len(inputs.shape) == 1:\n",
    "            print('single inference')\n",
    "            predictions = self.model['weights'].T * inputs + self.model['bias']\n",
    "            predictions = predictions.tolist()\n",
    "        else:\n",
    "            raise ValueError('invalid input shape')\n",
    "        return json.dumps(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the python inference model wrapper for the server\n",
    "model_wrapper = ModelWrapper()\n",
    "env = mlflow.pyfunc.get_default_conda_env()\n",
    "env = add_libraries_to_conda_env(env,libraries=['numpy'])\n",
    "\n",
    "# define the model weights randomly\n",
    "np_weights = np.random.rand(3,6)\n",
    "np_bias = np.random.rand(3)\n",
    "\n",
    "# checkpointing and logging the model in mlflow\n",
    "artifact_path = './np_model'\n",
    "np.save(artifact_path, {'weights':np_weights, 'bias':np_bias})\n",
    "model_artifacts = {\"model_path\" : artifact_path+'.npy'}\n",
    "mlflow.pyfunc.log_model(\"np_model\", python_model=model_wrapper, artifacts=model_artifacts, conda_env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying the model\n",
    "The above code logs a model in the experiments tab. For more info please refer [here](https://rocketml.gitbook.io/rocketml-user-guide/experiments). After deploying the model, we can obtain the model url for performing query as shown below.\n",
    "\n",
    "## 5. Query from the server\n",
    "\n",
    "There are two methods to perform query... The first is using `requests` library and the other using `curl` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.4653852 2.0398915 3.1586297]]\n",
      "[[2.5898142 1.8786722 2.5708313]\n",
      " [2.5593674 2.0349865 2.6073494]\n",
      " [3.6068127 2.184563  3.434891 ]\n",
      " [2.902356  2.3069386 2.953126 ]\n",
      " [2.637235  2.029517  2.6305063]\n",
      " [2.1237707 1.4357289 2.2697406]\n",
      " [3.0577552 1.9615693 3.0917268]\n",
      " [3.000336  2.0551164 2.9306874]\n",
      " [1.7696865 1.1558454 2.0506356]\n",
      " [2.4707701 1.7810833 2.9085393]\n",
      " [2.7724752 1.9395614 2.7040203]\n",
      " [3.3025193 2.3785317 3.3959422]\n",
      " [1.283288  0.5862225 1.4935229]\n",
      " [2.6822236 1.6249593 2.9523573]\n",
      " [2.6495163 1.9702814 2.6044328]\n",
      " [2.5158823 2.1459305 2.6397395]\n",
      " [2.6616333 1.9748728 2.554915 ]\n",
      " [2.0908048 1.3619525 2.3795984]\n",
      " [1.4215319 1.0770459 1.7504961]\n",
      " [2.0703692 1.4841241 2.2010443]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://oqgq98z.sciml.rocketml.net/invocations\"\n",
    "headers = {\"Content-Type\":\"text/csv\"}\n",
    "\n",
    "# First case, run inference on single data point\n",
    "np_array = np.random.rand(1,6).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "response = requests.post(url,data=json_data,headers=headers)\n",
    "if response.status_code == 200:\n",
    "    output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "    print(output)\n",
    "else:\n",
    "    print(response.status_code)\n",
    "    print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
    "\n",
    "# Second case, run inference on multiple data points\n",
    "np_array = np.random.rand(20,6).tolist()\n",
    "json_data = json.dumps(np_array)\n",
    "response = requests.post(url,data=json_data,headers=headers)\n",
    "if response.status_code == 200:\n",
    "    output = np.array(json.loads(response.json())).astype(np.float32)\n",
    "    print(output)\n",
    "else:\n",
    "    print(response.status_code)\n",
    "    print(\"REST API deployment is in progress -- please try again in a few minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: curl: command not found\n"
     ]
    }
   ],
   "source": [
    "!curl https://oqgq98z.sciml.rocketml.net/invocations -H 'Content-Type:text/csv' -d '[[0.6499166977064089, 0.17579454262114602, 0.2688911143313131, 0.7146591854799202, 0.6497433572112488, 0.7723469203958951]]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
