{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "4zsDWUk5cavn"
   },
   "source": [
    "# Deep Learning for Image Classification\n",
    "\n",
    "In this notebook, you will Learn how to build and train a convolutional neural network for image classification.\n",
    "\n",
    "## I. Training a classifier\n",
    "\n",
    "In this part, we will train a Convolutional Neural Network to classify images of 10 different classes (dogs, cats, car, ...) and see how our model performs on the test set.  \n",
    "\n",
    "\n",
    "## II. Exploring CNN Architectures\n",
    "\n",
    "This is the part where you get your hands dirty ;). Your mission is to experiment different CNN architectures and set hyperparameters in order to obtain the best accuracy on the test set!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgRltjas9PpN"
   },
   "source": [
    "The following command sets the backend of matplotlib to the 'inline' backend so that the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "GkjN23FKt2D-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAz-fhRRdFaR"
   },
   "source": [
    "### Plotting functions and useful imports\n",
    "\n",
    "You can skip this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nnee2WPudA9K"
   },
   "outputs": [],
   "source": [
    "# Python 2/3 compatibility\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Colors from Colorbrewer Paired_12\n",
    "colors = [[31, 120, 180], [51, 160, 44]]\n",
    "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    \"\"\"\n",
    "    :param img: (PyTorch Tensor)\n",
    "    \"\"\"\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5     \n",
    "    # Convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Color channel first -> color channel last\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "\n",
    "def plot_losses(train_history, val_history):\n",
    "    x = np.arange(1, len(train_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
    "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"Evolution of the training and validation loss\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    :param cm: (numpy matrix) confusion matrix\n",
    "    :param classes: [str]\n",
    "    :param normalize: (bool)\n",
    "    :param title: (str)\n",
    "    :param cmap: (matplotlib color map)\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(8, 8))   \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "oIONy65XZTrC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Import torch and create the alias \"th\"\n",
    "# instead of writing torch.name_of_a_method() , we only need to write th.name_of_a_method()\n",
    "# (similarly to numpy imported as np)\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "0kqEBjG6t2Eh"
   },
   "source": [
    "\n",
    "# II. Training a classifier\n",
    "\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset.\n",
    "There are 10 classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
    "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
    "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "\n",
    "![CIFAR10](http://pytorch.org/tutorials/_images/cifar10.png)\n",
    "\n",
    "\n",
    "Training an image classifier\n",
    "----------------------------\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1. Load and normalize the CIFAR10 training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a Convolution Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "UWTdj2uYcax7"
   },
   "source": [
    "### 1. Loading and normalizing CIFAR10 Dataset\n",
    "\n",
    "Using ``torchvision``, it’s extremely easy to load CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "KRrvrIi0t2Em"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "iX2ltR_zcayA"
   },
   "source": [
    "Seed the random generator to have reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "335xvR6acayB"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if th.cuda.is_available():\n",
    "  # Make CuDNN Determinist\n",
    "  th.backends.cudnn.deterministic = True\n",
    "  th.cuda.manual_seed(seed)\n",
    "\n",
    "# Define default device, we should use the GPU (cuda) if available\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "evFXNmbst2Ez"
   },
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalized range [-1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "91f6a440e0944b2fb63f9dc15c134f42",
      "8e341748ca9541df970cd0b552069e10",
      "1244edab2b954425abd6d1f9a21d87f3",
      "7b451aaa9bcb41f1bc4066c9d9c55c58",
      "781e8bc46fdf4ee89d34cfdf9eaa2d3f",
      "4057b0154f7143bca146c9282526fb0f",
      "6e2a39b4d929446bbae74f2fb74f64ce",
      "9a834bcb6dcd4b84afb0daa005941916"
     ]
    },
    "deletable": true,
    "editable": true,
    "id": "ZJ-hYN00t2E2",
    "outputId": "a784513c-74fd-40f6-b7af-8702e265c37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_367/4236094201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n\u001b[0;32m----> 9\u001b[0;31m                                         download=True, transform=transform)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m train_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch_size,\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files already downloaded and verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             raise ImportError(\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "num_workers = 2\n",
    "test_batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch_size,\n",
    "                                          num_workers=num_workers)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size,\n",
    "                                         num_workers=num_workers)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "n_training_samples = 40000 # Max: 50 000 - n_val_samples\n",
    "n_val_samples = 10000\n",
    "\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "# (In the last case, indexes do not need to account for training ones because the train=False parameter in datasets.CIFAR will select from the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "cGWVnBOft2FI"
   },
   "source": [
    "Let us show some of the training images, for fun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "deletable": true,
    "editable": true,
    "id": "68OfC35ut2FM",
    "outputId": "9d515c3c-0fb1-465c-f5f0-f8f732594cff"
   },
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('{:>10}'.format(classes[labels[j]]) for j in range(test_batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "8ULHEu5Zt2Fa"
   },
   "source": [
    "### 2. Define a Convolution Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "6k6rJyTTcayi"
   },
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "0JcmlEe8t2Fe"
   },
   "source": [
    "####  Forward propagation\n",
    "\n",
    "In PyTorch, there are built-in functions that carry out the convolution steps for you.\n",
    "\n",
    "- **nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0):** Convolution layer. You can read the full documentation [here](http://pytorch.org/docs/master/nn.html#conv2d)\n",
    "\n",
    "- **nn.MaxPool2d(kernel_size, stride=None, padding=0):** Max pooling layer. You can read the full documentation [here](http://pytorch.org/docs/master/nn.html#maxpool2d)\n",
    "\n",
    "- **F.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](http://pytorch.org/docs/master/nn.html#torch.nn.ReLU)\n",
    "\n",
    "- **x.view(new_shape)**: Returns a new tensor with the same data but different size. It is the equivalent of numpy function *reshape* (Gives a new shape to an array without changing its data). You can read the full documentation [here.](http://pytorch.org/docs/master/tensors.html#torch.Tensor.view)\n",
    "\n",
    "- **nn.Linear(in_features, out_features):** Applies a linear transformation to the incoming data: $y = Ax + b$, it is also called a fully connected layer. You can read the full documentation [here.](http://pytorch.org/docs/master/nn.html#linear-layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "rbykSRDTcaym"
   },
   "source": [
    "#### Simple Convolutional Neural Network\n",
    "\n",
    "ConvNet with one convolution layer followed by a max pooling operation,\n",
    "one fully connected layer and an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "X4pljAWycayn"
   },
   "outputs": [],
   "source": [
    "class SimpleConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvolutionalNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # cf comments in forward() to have step by step comments\n",
    "        # on the shape (how we pass from a 3x32x32 input image to a 18x16x16 volume)\n",
    "        self.fc1 = nn.Linear(18 * 16 * 16, 64) \n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass,\n",
    "        x shape is (batch_size, 3, 32, 32)\n",
    "        (color channel first)\n",
    "        in the comments, we omit the batch_size in the shape\n",
    "        \"\"\"\n",
    "        # shape : 3x32x32 -> 18x32x32\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # 18x32x32 -> 18x16x16\n",
    "        x = self.pool(x)\n",
    "        # 18x16x16 -> 4608\n",
    "        x = x.view(-1, 18 * 16 * 16)\n",
    "        # 4608 -> 64\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 64 -> 10\n",
    "        # The softmax non-linearity is applied later (cf createLossAndOptimizer() fn)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "2SQi9Xf-t2Fu"
   },
   "source": [
    "### 3. Define a loss function and optimizer\n",
    "\n",
    "Let's use a Classification Cross-Entropy loss and ADAM (optionally, SGD with momentum). You can read more about  [optimization methods](https://pytorch.org/docs/stable/optim.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "DOUiPtZQt2Fx"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    # it combines softmax with negative log likelihood loss\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "saJW5bKRt2F9"
   },
   "source": [
    "### 4. Train the network\n",
    "\n",
    "\n",
    "This is when things start to get interesting.\n",
    "We simply have to loop over our data iterator, feed the inputs to the network, and optimize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "mNf1e8QZcay1"
   },
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "EqDD8_z8cay2"
   },
   "outputs": [],
   "source": [
    "def get_train_loader(batch_size):\n",
    "    return torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "# Use larger batch size for validation to speed up computation\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler,\n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "yTDHHbLpcay5"
   },
   "source": [
    "#### Training loop\n",
    "The training script: it takes ~10s per epoch with batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "dATbDR5pt2GE"
   },
   "outputs": [],
   "source": [
    "def train(net, batch_size, n_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a neural network and print statistics of the training\n",
    "    \n",
    "    :param net: (PyTorch Neural Network)\n",
    "    :param batch_size: (int)\n",
    "    :param n_epochs: (int)  Number of iterations on the training set\n",
    "    :param learning_rate: (float) learning rate used by the optimizer\n",
    "    \"\"\"\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"n_epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_minibatches = len(train_loader)\n",
    "\n",
    "    criterion, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    # Init variables used for plotting the loss\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    best_error = np.inf\n",
    "    best_model_path = \"best_model.pth\"\n",
    "    \n",
    "    # Move model to gpu if possible\n",
    "    net = net.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        print_every = n_minibatches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Move tensors to correct device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # print every 10th of epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:    \n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
    "                      time.time() - start_time))\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "\n",
    "        train_history.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        total_val_loss = 0\n",
    "        # Do a pass on the validation set\n",
    "        # We don't need to compute gradient,\n",
    "        # we save memory and computation using th.no_grad()\n",
    "        with th.no_grad():\n",
    "          for inputs, labels in val_loader:\n",
    "              # Move tensors to correct device\n",
    "              inputs, labels = inputs.to(device), labels.to(device)\n",
    "              # Forward pass\n",
    "              predictions = net(inputs)\n",
    "              val_loss = criterion(predictions, labels)\n",
    "              total_val_loss += val_loss.item()\n",
    "            \n",
    "        val_history.append(total_val_loss / len(val_loader))\n",
    "        # Save model that performs best on validation set\n",
    "        if total_val_loss < best_error:\n",
    "            best_error = total_val_loss\n",
    "            th.save(net.state_dict(), best_model_path)\n",
    "\n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "\n",
    "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "    \n",
    "    # Load best model\n",
    "    net.load_state_dict(th.load(best_model_path))\n",
    "    \n",
    "    return train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": true,
    "editable": true,
    "id": "cJX2anB5cay_",
    "outputId": "1042a5f9-3417-4f25-85a6-6dffb0739349"
   },
   "outputs": [],
   "source": [
    "net = SimpleConvolutionalNetwork()\n",
    "\n",
    "train_history, val_history = train(net, batch_size=32, n_epochs=15, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "UkVKNPtccazC"
   },
   "source": [
    "Now, let's look at the evolution of the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "deletable": true,
    "editable": true,
    "id": "4CUQt-HJcazF",
    "outputId": "b7e7805c-2b1f-4de5-9b77-65c6cf9ab7fe"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_history, val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "O90WcUTwt2GU"
   },
   "source": [
    "### 5. Test the network on the test data\n",
    "\n",
    "\n",
    "We have trained the network for 2 passes over the training dataset.\n",
    "But we need to check if the network has learnt anything at all.\n",
    "\n",
    "We will check this by predicting the class label that the neural network\n",
    "outputs, and checking it against the ground-truth. If the prediction is\n",
    "correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display an image from the test set to get familiar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "deletable": true,
    "editable": true,
    "id": "V4vljwBlt2GX",
    "outputId": "0b981673-aadf-4bb8-a58b-5bcedf63c376"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  images, labels = next(iter(test_loader))\n",
    "except EOFError:\n",
    "  pass\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"Ground truth:\\n\")\n",
    "\n",
    "print(' '.join('{:>10}'.format(classes[labels[j]]) for j in range(test_batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "KpmaQT4Zt2Gn"
   },
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": true,
    "editable": true,
    "id": "utIfocFrt2Gs",
    "outputId": "94f9ee79-e2fc-4f48-ceac-bab65469a31a"
   },
   "outputs": [],
   "source": [
    "outputs = net(images.to(device))\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "6mU42O0Gt2G2"
   },
   "source": [
    "The outputs are energies for the 10 classes.\n",
    "The higher the energy for a class, the more the network\n",
    "thinks that the image is from that particular class.\n",
    "So, let's get the index of the highest energy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "deletable": true,
    "editable": true,
    "id": "IWTWHHs9t2G5",
    "outputId": "18dbbb35-e462-4893-b814-179230b67a43"
   },
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(\"Predicted:\\n\")\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "print(' '.join('{:>10}'.format(classes[predicted[j]]) for j in range(test_batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "AUpCEAOTt2HK"
   },
   "source": [
    "The results seem pretty good.\n",
    "\n",
    "Let us look at how the network performs on the whole test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": true,
    "editable": true,
    "id": "LI6JtYwTt2HM",
    "outputId": "21302c6b-5697-47f4-daa9-82508a8edd6e"
   },
   "outputs": [],
   "source": [
    "def dataset_accuracy(net, data_loader, name=\"\"):\n",
    "    net = net.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    accuracy = 100 * float(correct) / total\n",
    "    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n",
    "\n",
    "def train_set_accuracy(net):\n",
    "    dataset_accuracy(net, train_loader, \"train\")\n",
    "\n",
    "def val_set_accuracy(net):\n",
    "    dataset_accuracy(net, val_loader, \"validation\")  \n",
    "    \n",
    "def test_set_accuracy(net):\n",
    "    dataset_accuracy(net, test_loader, \"test\")\n",
    "\n",
    "def compute_accuracy(net):\n",
    "    train_set_accuracy(net)\n",
    "    val_set_accuracy(net)\n",
    "    test_set_accuracy(net)\n",
    "    \n",
    "print(\"Computing accuracy...\")\n",
    "compute_accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "iGGyra-4t2HW"
   },
   "source": [
    "That initial 63.97 % on the test set of images looks waaay better than chance, which is 10% accuracy (randomly picking\n",
    "a class out of 10 classes).\n",
    "Seems like the network learnt something.\n",
    "As a baseline, a linear model achieves around 30% accuracy.\n",
    "\n",
    "What are the classes that performed well, and the classes that did not perform well?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": true,
    "editable": true,
    "id": "rkim9_INt2HY",
    "outputId": "f4efd223-0480-4810-9d0c-8ef237edbec8"
   },
   "outputs": [],
   "source": [
    "def accuracy_per_class(net):\n",
    "    net = net.to(device)\n",
    "    n_classes = 10\n",
    "    # (real, predicted)\n",
    "    confusion_matrix = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for i in range(predicted.size(0)):\n",
    "            confusion_matrix[labels[i], predicted[i]] += 1\n",
    "            label = labels[i]\n",
    "\n",
    "    print(\"{:<10} {:^10}\".format(\"Class\", \"Accuracy (%)\"))\n",
    "    for i in range(n_classes):\n",
    "        class_total = confusion_matrix[i, :].sum()\n",
    "        class_correct = confusion_matrix[i, i]\n",
    "        percentage_correct = 100.0 * float(class_correct) / class_total\n",
    "        \n",
    "        print('{:<10} {:^10.2f}'.format(classes[i], percentage_correct))\n",
    "    return confusion_matrix\n",
    "\n",
    "confusion_matrix = accuracy_per_class(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "AZKLymOacazg"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "ekJHz3vpcazg"
   },
   "source": [
    "Let's look at what type of error our networks makes... \n",
    "It seems that our network is pretty good at classifying ships,\n",
    "but has some difficulties to differentiate cats and dogs.\n",
    "Also, it classifies a lot of trucks as cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": true,
    "editable": true,
    "id": "1aYMqD1Ocazi",
    "outputId": "cbc11340-c638-4aea-b37e-b61789558cc5"
   },
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(confusion_matrix, classes, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(confusion_matrix, classes,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "MVv-mV8Pt2Hs"
   },
   "source": [
    "# III. Exploring CNN Architectures\n",
    "\n",
    "Now, it is your turn to build a Convolutional Neural Network. The goal of this section is to explore different CNN architectures and set hyperparameters in order to obtain the best accuracy on the **test** set!\n",
    "\n",
    "The network that you have to tweak is called **MyConvolutionalNetwork**.\n",
    "\n",
    "You can start changing the batch_size, number of epochs and then try adding more convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "h1blK9eicazo"
   },
   "source": [
    "### PyTorch functions to build the network\n",
    "- **nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0):** Convolution layer. You can read the full documentation [here](http://pytorch.org/docs/master/nn.html#conv2d)\n",
    "\n",
    "- **nn.MaxPool2d(kernel_size, stride=None, padding=0):** Max pooling layer. You can read the full documentation [here](http://pytorch.org/docs/master/nn.html#maxpool2d)\n",
    "\n",
    "- **F.relu(Z1):** computes the element-wise ReLU of Z1 (which can be of any shape). You can read the full documentation [here.](http://pytorch.org/docs/master/nn.html#torch.nn.ReLU)\n",
    "\n",
    "- **x.view(new_shape)**: Returns a new tensor with the same data but different size. It is the equivalent of numpy function *reshape* (Gives a new shape to an array without changing its data.). You can read the full documentation [here.](http://pytorch.org/docs/master/tensors.html#torch.Tensor.view)\n",
    "\n",
    "- **nn.Linear(in_features, out_features):** Applies a linear transformation to the incoming data: $y = Ax + b$, it is also called a fully connected (fc) layer. You can read the full documentation [here.](http://pytorch.org/docs/master/nn.html#linear-layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "a8-lKBaacazp"
   },
   "source": [
    "**Convolution Formulas**:\n",
    "\n",
    "The formulas relating the output shape $(C_2, H_2, W_2)$ of the convolution to the input shape $(C_1, H_1, W_1)$ are:\n",
    "\n",
    "\n",
    "$$ H_2 = \\lfloor \\frac{H_1 - kernel\\_size + 2 \\times padding}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ W_2 = \\lfloor \\frac{W_1 - kernel\\_size + 2 \\times padding}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ C_2 = \\text{number of filters used in the convolution}$$\n",
    "\n",
    "NOTE: $C_2 = C_1$ in the case of max pooling\n",
    "\n",
    "where:\n",
    "- $H_2$: height of the output volume  \n",
    "- $W_2$: width of the output volume  \n",
    "- $C_1$: in_channels, number of channels in the input volume\n",
    "- $C_2$: out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "acppf3nkcazr"
   },
   "outputs": [],
   "source": [
    "def get_output_size(in_size, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Get the output size given all the parameters of the convolution\n",
    "    :param in_size: (int) input size\n",
    "    :param kernel_size: (int)\n",
    "    :param stride: (int)\n",
    "    :param paddind: (int)\n",
    "    :return: (int)\n",
    "    \"\"\"\n",
    "    return int((in_size - kernel_size + 2 * padding) / stride) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "SEsbZoTOcazu"
   },
   "source": [
    "#### Example of use of helper method get_output_size() \n",
    "\n",
    "Let's assume you have an *input volume of size 3x32x32* (where 3 is the number of channels)\n",
    "and you use a 2D convolution with the following parameters:\n",
    "\n",
    "```python\n",
    "conv1 = nn.Conv2d(3, 18, kernel_size=7, stride=2, padding=1)\n",
    "```\n",
    "then, the size of the output volume is 18x?x? (because we have 18 filters) where ? is given by the convolution formulas (see above).\n",
    "\n",
    "**get_output_size()** function allows to compute that size:\n",
    "\n",
    "```\n",
    "out_size = get_output_size(in_size=32, kernel_size=7, stride=2, padding=1)\n",
    "print(out_size) # prints 14\n",
    "```\n",
    "\n",
    "That is to say, *the output volume is 18x14x14*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "deletable": true,
    "editable": true,
    "id": "2JFQ1wgKcazv",
    "outputId": "8b4378cc-5f22-4147-98a7-d11582674e7c"
   },
   "outputs": [],
   "source": [
    "out_size = get_output_size(in_size=32, kernel_size=3, stride=1, padding=1)\n",
    "print(out_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "wviV5iQIcazz"
   },
   "source": [
    "Below is the neural network you have to edit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "fnKUPUDTcaz1"
   },
   "outputs": [],
   "source": [
    "class MyConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyConvolutionalNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, stride=1, padding=1) # 6 32 32\n",
    "        self.bn1 = torch.nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 10, kernel_size=3, stride=1, padding=1)  # 10 32 32\n",
    "        self.bn2 = torch.nn.BatchNorm2d(10)\n",
    "        self.conv3 = nn.Conv2d(10, 16, kernel_size=3, stride=1, padding=1)  # 16 32 32\n",
    "        self.bn3 = torch.nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # 16 16 16 \n",
    "        self.conv4 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)   # 32 16 16\n",
    "        self.bn4 = torch.nn.BatchNorm2d(16)\n",
    "        self.dp1 = torch.nn.Dropout2d(0.2)\n",
    "        self.conv5 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)  # 64 14 14\n",
    "        self.bn5 = torch.nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) # 64 7 7\n",
    "\n",
    "        #### START CODE: ADD NEW LAYERS ####\n",
    "        # (do not forget to update `flattened_size`:\n",
    "        # the input size of the first fully connected layer self.fc1)\n",
    "        # self.conv2 = ...\n",
    "        \n",
    "        # Size of the output of the last convolution:\n",
    "        self.flattened_size = 64 * 7 * 7\n",
    "        ### END CODE ###\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattened_size, 64)\n",
    "        self.dp2 = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass,\n",
    "        x shape is (batch_size, 3, 32, 32)\n",
    "        (color channel first)\n",
    "        in the comments, we omit the batch_size in the shape\n",
    "        \"\"\"\n",
    "        # shape : 3x32x32 -> 18x32x32\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        #### START CODE: USE YOUR NEW LAYERS HERE ####\n",
    "        # x = ...\n",
    "        \n",
    "        #### END CODE ####\n",
    "        \n",
    "        # Check the output size\n",
    "        output_size = np.prod(x.size()[1:])\n",
    "        assert output_size == self.flattened_size,\\\n",
    "                \"self.flattened_size is invalid {} != {}\".format(output_size, self.flattened_size)\n",
    "        \n",
    "        # 18x16x16 -> 4608   (64 * 7 * 7 = 3136)\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        # 4608 -> 64\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 64 -> 10\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": true,
    "editable": true,
    "id": "ruLWyTSocaz5",
    "outputId": "08a3d3c4-2155-4445-dcf8-d6b9877f5ec4"
   },
   "outputs": [],
   "source": [
    "net = MyConvolutionalNetwork()\n",
    "train_history, val_history = train(net, batch_size=128, n_epochs=10, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "u7cgVbkDcaz9"
   },
   "source": [
    "### Losses Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "deletable": true,
    "editable": true,
    "id": "XtXu67qbcaz-",
    "outputId": "27b6b764-ca27-4cde-cca8-2d5ca283e261"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_history, val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "TuGKgAMWcaz_"
   },
   "source": [
    "### Accuracy of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "deletable": true,
    "editable": true,
    "id": "TWowqQhYca0B",
    "outputId": "5e938dab-0e6f-41fe-f0cb-4ef293562b1a"
   },
   "outputs": [],
   "source": [
    "compute_accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "st9f_4opca0F"
   },
   "source": [
    "**Baseline: Simple Convolutional Neural Network (form part II)**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>Accuracy on the test set:</td>\n",
    "    <td>63.97 %</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "deletable": true,
    "editable": true,
    "id": "OKvmb4p-ca0I",
    "outputId": "f88790b4-227c-4eae-d47b-7276868fd257"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = accuracy_per_class(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "deletable": true,
    "editable": true,
    "id": "ih5Pj0WBca0L",
    "outputId": "2c353ebd-9298-43e7-8c1d-cfc42ce74d37"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix, classes,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "7xzIkZqit2IA"
   },
   "source": [
    "### Going further\n",
    "\n",
    "- [Coursera Course on CNN](https://www.coursera.org/learn/convolutional-neural-networks)\n",
    "- [Standford Course](http://cs231n.stanford.edu/syllabus.html)\n",
    "- [PyTorch Tutorial](http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "- [How backpropagation works](neuralnetworksanddeeplearning.com/chap2.html) (Michael )\n",
    "\n",
    "If you feel like this was too easy peasy:\n",
    "\n",
    "-Investigate further [optimization methods](https://pytorch.org/docs/stable/optim.html) beyond SGD, and Adam and their parameters.\n",
    "\n",
    "-Look at ways to improve your network using regularization techniques\n",
    "\n",
    "-Look at ways to visualize network activations for model interpretability\n",
    "\n",
    "-Use transfer learning, in order to use torchvision with pretrained=True with some pretrained models\n",
    "\n",
    "\n",
    "Note: This tutorial is based on the [original PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) and was adapted by [Antonin Raffin](http://araffin.github.io/) for the ROB313 course at ENSTA ParisTech. Thanks to Clement Pinard for feedback!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEFW6M6jZtWk"
   },
   "source": [
    "### More documentation/ questions to explore about Google Colab: \n",
    "\n",
    "-How to connect your Google Drive with Google Colab?\n",
    "\n",
    "-How to import a new notebook and save it to your GDrive?\n",
    "\n",
    "-How to use files which are contained in your GDrive?\n",
    "\n",
    "Some tips [here](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Extras to read later\n",
    "### Visualizing Convolution parameters:\n",
    "[A guide to convolution arithmetic for deep learning](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) \n",
    "by Vincent Dumoulin, Francesco Visin \n",
    "\n",
    "\n",
    "### Documentation of autograd and Function: \n",
    "[Autograd](http://pytorch.org/docs/autograd)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cifar10_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1244edab2b954425abd6d1f9a21d87f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4057b0154f7143bca146c9282526fb0f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_781e8bc46fdf4ee89d34cfdf9eaa2d3f",
      "value": 1
     }
    },
    "4057b0154f7143bca146c9282526fb0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e2a39b4d929446bbae74f2fb74f64ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "781e8bc46fdf4ee89d34cfdf9eaa2d3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7b451aaa9bcb41f1bc4066c9d9c55c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a834bcb6dcd4b84afb0daa005941916",
      "placeholder": "​",
      "style": "IPY_MODEL_6e2a39b4d929446bbae74f2fb74f64ce",
      "value": " 170500096/? [00:20&lt;00:00, 88419771.85it/s]"
     }
    },
    "8e341748ca9541df970cd0b552069e10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f6a440e0944b2fb63f9dc15c134f42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1244edab2b954425abd6d1f9a21d87f3",
       "IPY_MODEL_7b451aaa9bcb41f1bc4066c9d9c55c58"
      ],
      "layout": "IPY_MODEL_8e341748ca9541df970cd0b552069e10"
     }
    },
    "9a834bcb6dcd4b84afb0daa005941916": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
